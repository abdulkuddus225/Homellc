# -*- coding: utf-8 -*-
"""Homellc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VIWbCX8zmcM4d1YfOTDf0MxuW4LAN9gR
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
import seaborn as sns

df = pd.read_csv('drive/MyDrive/HomeLLC/HomeLLC.csv')
df.head(12)

# Preprocessing Data
def Preprocessing(df):
  for label, content in df.items():
    if not pd.api.types.is_numeric_dtype(content):
      df[label] = pd.Categorical(content).codes + 1
  return df
Preprocessing(df)



df = df.rename(columns={'Economic Growth':'Economic_Growth',
                        'Unemployment Rate':'Unemployment_Rate',
                        'Interest Rate':'Interest_Rate',
                        'House Price':'House_Price'})
df

"""# Visualizing data using graphs for better understanding of features"""

sns.pairplot(df, x_vars=['Unemployment_Rate','Interest_Rate','House_Price'], y_vars='Economic_Growth',kind='reg', height=5, aspect=0.7)

df.corr()

# This matrix shows the relation between all the features
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(15,10))
ax = sns.heatmap(corr_matrix, 
                annot=True,
                linewidths=0.5,
                fmt=".2f",
                cmap="YlGnBu");

"""Each square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1. Values closer to zero means there is no linear trend between the two variables. The close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases. The diagonals are all 1/dark green because those squares are correlating each variable to itself (so it's a perfect correlation). For the rest the larger the number and darker the color the higher the correlation between the two variables. The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares.

# Modelling
    * As this is a regression problem because house price is a continuous variable we need regression algorithms.
    So the first we will use Linear Regression
    * After that if the score is low will go for scikit-learn model selection chart. https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html
"""

# Modelling Using Linear Regression
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_log_error
np.random.seed(42)
LR_model = LinearRegression()
X = df.drop("House_Price",axis=1)
y = df["House_Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2)
LR_model.fit(X_train, y_train)
LR_model.score(X_test, y_test)
y_preds = LR_model.predict(X_test)
mean_squared_log_error(y_test, y_preds)

LR_model.score(X_test, y_test)

from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
np.random.seed(42)
model = XGBRegressor(objective='reg:squarederror')
X = df.drop("House_Price",axis=1)
y = df["House_Price"]

X_train, X_test, y_train, y_test = train_test_split(X, y , test_size=0.2)
model.fit(X_train, y_train)

model.score(X_test,y_test)

y_preds = model.predict(X_test)

from sklearn.metrics import mean_squared_error

def RMSLE(y_test, y_preds):
  return np.sqrt(mean_squared_error(y_test, y_preds))

def show_scores(model):
  scores = {
      "Root Mean Squared Log  Error" : RMSLE(y_test, y_preds)
  }
  return scores

show_scores(model)

model.feature_importances_

# Plotting Feature Importance

def plotting(columns, importances, n=20):
  df = (pd.DataFrame({"features": columns,
                      "feature_importance": importances})
                    .sort_values("feature_importance", ascending=False)
                    .reset_index(drop=True))
  fig, ax = plt.subplots()
  ax.barh(df["features"][:n], df["feature_importance"][:20])
  ax.set_xlabel("Features")
  ax.set_ylabel("Feature Importance")
  ax.invert_yaxis()

plotting(X_train.columns, model.feature_importances_)

y_preds

y_test

plt.scatter(y_test,y_preds)
plt.xlabel("Original Value")
plt.ylabel("Predicted Value")
plt.title("Original Vs Predicted Value");

"""# Evaluation Machine Learning Model"""

# Mean Absolute Error
from sklearn.metrics import mean_absolute_error
mean_absolute_error(y_preds, y_test)

from sklearn.model_selection import cross_val_score
scores = cross_val_score(model, X, y, cv=5)
scores

print("%0.2f accuracy with a standard deviation of %0.2f" % (scores.mean(), scores.std()))

"""#This Concludes the Machine Learning Model for US Housing Price Prediction.
#The result shows that the Model is giving us the R2 score of 99%. 
#The accuracy is -3.88 because the model is not predicting actual house prices but it is giving us approximately correct results.
"""

